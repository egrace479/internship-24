{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1f6N8Mh4tbTtjqcEohgjE9ijPrs8mTR73","timestamp":1684191346731}],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Clustering\n","\n","\n","\n","\n","\n"],"metadata":{"id":"C6BVvIVDVzrN"}},{"cell_type":"code","source":["# The K-Medoids algorithm isn't included in scikit-learn, so we have to install an additional package\n","!pip install scikit-learn-extra -qq"],"metadata":{"id":"m80KD1AHrb3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Data"],"metadata":{"id":"ry6Jxg0lrVgF"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs, make_circles, make_moons\n","\n","# Generate some random data \n","# Uncomment the following lines to see how K-Means performs on different types of patterns in the data\n","# Also feel free to experiment with changing the factor and noise parameters\n","# The random_state parameter is set so you can replicate results, but feel free to change this or unset it to get different patterns\n","\n","X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n","# X, y = make_circles(n_samples=100, factor=.5, noise=.05, random_state=42)\n","# X, y = make_moons(n_samples=100, noise=.05, random_state=42)"],"metadata":{"id":"vS1ADYCzRN9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Choose the clustering algorithm"],"metadata":{"id":"-omdK9IcrNom"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn_extra.cluster import KMedoids\n","import matplotlib.pyplot as plt\n","\n","# Choose the clustering algorithm by commenting/uncommenting one of the lines below\n","# Experiment with changing the value of n_clusters; observe how the predictions change\n","# How does the algorithm behave when this doesn't match the true number of clusters in the data?\n","\n","clusterer = KMeans(n_clusters=2)\n","# clusterer = KMedoids(n_clusters=2)\n","\n","clusterer.fit(X) # Run the clustering algorithm \n","labels = clusterer.predict(X)  # get the predicted cluster labels for the data\n","\n","# Visualize results\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# True labels plot\n","scatter = ax1.scatter(X[:, 0], X[:, 1], c=y)\n","ax1.set_title(\"True Labels\")\n","\n","# Predicted labels plot\n","scatter = ax2.scatter(X[:, 0], X[:, 1], c=labels)\n","ax2.set_title(\"Predicted Labels\")\n","\n","# Hide X and Y axes tick marks\n","ax1.set_xticks([])\n","ax1.set_yticks([])\n","ax2.set_xticks([])\n","ax2.set_yticks([])\n","\n","plt.show()"],"metadata":{"id":"keLzBsCyVymE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Neural Networks"],"metadata":{"id":"H-qlmsW8NZmW"}},{"cell_type":"code","source":["import torch\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"BS2SHiHyES78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the hyperparameters\n","\n","input_size = 784  # One input for each pixel (28 x 28 = 784)\n","hidden_size = 128 # We choose to have a hidden layer of 128 neurons\n","output_size = 10  # Predict the probability for each class (digit)\n","\n","batch_size = 128\n","num_epochs = 10"],"metadata":{"id":"7nOtZReKDg5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing the data"],"metadata":{"id":"daSs6MLhHtjE"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","\n","# Define the transform to normalize the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.1307,), (0.3081,)) # Feel free to uncomment this line to see how the results are affected (or not affected)\n","])"],"metadata":{"id":"fmuk1vqiDlNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.datasets as datasets\n","\n","# Load the MNIST dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","\n","# Create the data loaders\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"Acjwl4GrDnni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defining the network"],"metadata":{"id":"k3uZuX-VHwxT"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# Define the neural network architecture\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)   # This holds the weights and biases for the first fully connected layer\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = x.view(-1, input_size)  \n","        x = self.fc1(x)\n","        x = torch.relu(x)  # ReLU nonlinearity\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"gm2g1NC5Dp53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the neural network\n","mlp = MLP().to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Experiment with different optimizers by uncommenting the following lines\n","# You can also see how the optimizer parameters affect training\n","# It's a good idea to change the name of the run in the following cell to keep track of which optimizer and parameter setting you used\n","\n","optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1, momentum=0, weight_decay=0)\n","# optimizer = torch.optim.Adagrad(mlp.parameters(), lr=0.1)\n","# optimizer = torch.optim.RMSprop(mlp.parameters(), lr=0.01)\n","# optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n","# optimizer = torch.optim.AdamW(mlp.parameters(), lr=0.01)"],"metadata":{"id":"WmuGDoBxDtXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","# Tensorboard is a logging utility\n","# Here, we just use it to plot the loss, though it has many more capabilities\n","\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter('logs/first_run') # Change the name of the run here to keep track of your experiments in Tensorboard"],"metadata":{"id":"__GNjPcS-Rp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training the network"],"metadata":{"id":"OX9yXcuNIBLZ"}},{"cell_type":"code","source":["%tensorboard --logdir logs"],"metadata":{"id":"FxXm4FxWA-BU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXUoaspRC1hs"},"outputs":[],"source":["# Train the neural network\n","\n","total_steps = 0\n","\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        total_steps += 1\n","\n","        # Move the data to the GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = mlp(images)  # Forward pass through the network\n","\n","        loss = loss_fn(outputs, labels)  # Compute the loss\n","        \n","\n","        optimizer.zero_grad()\n","        loss.backward()  # Compute gradients (\"backward\" refers to the method of computing gradients by making a backward pass through the network)\n","        optimizer.step()  # Update the parameters with one step of gradient descent \n","\n","        # Print the loss every 100 iterations\n","        if (i + 1) % 100 == 0:\n","            writer.add_scalar(\"Loss\", loss, total_steps)\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')  # .item() takes a one-element tensor and returns the value\n","\n","writer.close()"]},{"cell_type":"markdown","source":["## Evaluating the Network"],"metadata":{"id":"nIPPo1K8IEfT"}},{"cell_type":"code","source":["# Test the neural network\n","\n","mlp.eval()\n","\n","with torch.no_grad():  # This line disables gradient computation, which PyTorch does automatically by default. Since we're not training, we dont them, so this speeds things up slightly.\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      \n","        # Move the data to the GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = mlp(images)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the class predicted with the greatest probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print(f'Test Accuracy: {100 * correct / total}%')  # The f'' syntax is a \"format string,\" which allows us to inject expressions using {}. It's just a nice convenience for easier printing"],"metadata":{"id":"GRPnXt5sDd7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Just get 10 images to display\n","visual_examples = torch.utils.data.Subset(test_dataset, range(0, 10))  \n","visual_loader = torch.utils.data.DataLoader(dataset=visual_examples, batch_size=1, shuffle=True)\n","\n","\n","for images, labels in visual_loader:\n","    # Move the data to the GPU if available\n","    images = images.to(device)\n","\n","    # Make a prediction\n","    outputs = mlp(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    prediction = predicted.item()\n","    truth = labels.item()\n","\n","    # Display the image and the prediction\n","    plt.imshow(images.cpu().numpy()[0][0],cmap='gray')\n","    plt.title(f'Prediction: {prediction}, Truth: {truth}')\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"X-Srv9izDxtL"},"execution_count":null,"outputs":[]}]}